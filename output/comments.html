<div class='commentURL'>/2008/09/11/tonchidot-is-it-real-or-just-a-video-that-implies-more-than-it-does/</div>
<span id='oldComments'>
<h2>1 Archived Comment</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, January 25, 2016, UTC, http://nimue-new.cyberstreet.c said:</span>
  <span class='text'><p>I was curious if you ever thought of changing the page layout of your site?<br>Its very well written; I love what youve got to say. But maybe you could a little more in the way of <br>content so people could connect with it better. Youve got an awful lot of text for only having 1 or 2 images.</p><p>Maybe you could space it out better?</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2008/09/14/is-playing-spore-in-the-eagle-pub-a-form-of-ar/</div>
<span id='oldComments'>
<h2>12 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Sunday, December 16, 2012, UTC, Ha Nam said:</span>
  <span class='text'><p>Why do you think the answer is no</p></span>
</li>
<li>
  <span class='source'> on Wednesday, May 20, 2015, UTC, tour du lich thai lan gia re said:</span>
  <span class='text'><p>why do you thing that?</p></span>
</li>
<li>
  <span class='source'> on Sunday, December 13, 2015, UTC, market experts said:</span>
  <span class='text'><p>It's nearly impossible to find knowledgeable people on this subject, <br>however, you seem like you know what you're talking about!<br>Thanks</p></span>
</li>
<li>
  <span class='source'> on Monday, January 11, 2016, UTC, Lyn said:</span>
  <span class='text'><p>Good day! I could have sworn I've visited this blog before but after browsing <br>through a few of the posts I realized it's new to me. Nonetheless, I'm certainly pleased I came across it and I'll <br>be book-marking it and checking back frequently!</p></span>
</li>
<li>
  <span class='source'> on Friday, January 15, 2016, UTC, best bay st louis real estate  said:</span>
  <span class='text'><p>Howdy! I know this is kinda off topic but I was wondering if you knew where I could locate <br>a captcha plugin for my comment form? I'm using the same <br>blog platform as yours and I'm having difficulty finding <br>one? Thanks a lot!</p></span>
</li>
<li>
  <span class='source'> on Friday, January 15, 2016, UTC, Dean Graziosi said:</span>
  <span class='text'><p>Very good info. Lucky me I recently found your blog by accident (stumbleupon).</p><p>I've saved it for later!</p></span>
</li>
<li>
  <span class='source'> on Friday, January 15, 2016, UTC, whois.scansafe.net said:</span>
  <span class='text'><p>Howdy! Would you mind if I share your blog with my twitter group?<br>There's a lot of people that I think would really enjoy your content.<br>Please let me know. Cheers</p></span>
</li>
<li>
  <span class='source'> on Saturday, January 16, 2016, UTC, buy xanax online said:</span>
  <span class='text'><p>I'm extremely impressed with your writing skills as well as with <br>the layout on your weblog. Is this a paid thbeme oor did you morify itt yourself?</p><p>Anyway keep up the nice quality writing, it's rare to see a nice blog <br>like this one nowadays.</p></span>
</li>
<li>
  <span class='source'> on Saturday, January 16, 2016, UTC, plastic extrusion machine said:</span>
  <span class='text'><p>Another advantage is the safety feature. In the cross over or perhaps soften step, the main width within the <br>attach heightens even though the peak on the flight journey will <br>reduce. The plastic extruder consists from the extrusion device, the extrusion and pressing <br>system, the transmission system and the heating and cooling system.</p></span>
</li>
<li>
  <span class='source'> on Sunday, January 24, 2016, UTC, best bay st louis realtor said:</span>
  <span class='text'><p>I don't know whether it's just me or if everyone else encountering problems with <br>your blog. It looks like some of the text within your posts are running off the <br>screen. Can somebody else please provide feedback and let me know if this is <br>happening to them as well? This might be a issue with my browser because I've had this happen before.<br>Thanks</p></span>
</li>
<li>
  <span class='source'> on Sunday, January 24, 2016, UTC, PlagiarismCheckerOnline said:</span>
  <span class='text'><p>Great web site you've got here.. It's hard to find <br>high quality writing like yours these days. I seriously appreciate people like you!<br>Take care!!</p></span>
</li>
<li>
  <span class='source'> on Monday, January 25, 2016, UTC, google docs word counter said:</span>
  <span class='text'><p>Just wish to say your article is as amazing.<br>The clearness for your post is just nice and that i can suppose you're an expert in this subject.</p><p>Fine along with your permission let me to grab your RSS feed to keep updated with forthcoming post.</p><p>Thank you a million and please keep up the rewarding work.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2008/09/26/hoping-to-avoid-the-hype-bubble/</div>
<span id='oldComments'>
<h2>2 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Saturday, September 27, 2008, UTC, twDarkflame said:</span>
  <span class='text'><p>The "mind-boggling" amount of data you want is already there.<br>Almost every product sold in the western world has a barcode.<br>Cross-referanced with the stores database, and possibly something like google product search, and you have everything you need.<br>All we need from a technical standpoint is to have barcodes visible for AR devices to see.<br>The rest is "just" co-operation and establishing of standards.</p><p>You are right that a closed system stands little chance of success, but from a technical standpoint of a lot of the "AR Dream" is quite possible.</p><p>Especialy as the "point to look up a product" is just a tiny aspex of the overall usefullness of a decent AR device.</p></span>
</li>
<li>
  <span class='source'> on Saturday, September 27, 2008, UTC, blair said:</span>
  <span class='text'><p>I totally agree that some of the data we need is there;  yes, we can identify products, IF a device can read the barcode.  And, sure, it's possible to know what's in a store IF you can get access to the store's databases.</p><p>However, those are some huge IF's.  And some huge leaps of faith.  For example, name one store that keeps a SPATIAL record (computerized, not just implied or ad hoc) record of EXACTLY (3D coordinates, please) all those items are.  You might be able to find out what is in the store, but not where it is.  How do you know what is in the window?</p><p>Some folks think RFID will be the answer, but it's been field tested and largely has failed.  Big companies have eschewed putting RFID tags on products because of the cost and lack of benefit to them (as opposed to benefit to the stores).</p><p>Do some exercises for yourself to try out how big a leap you are making:<br>- go to a store and look in the window, and see how easy it is to see the barcodes on the display items.  Bring the best DSLR camera you can get (e.g., I have looked at pictures taken with my Nikon D300).  Zoom in;  sharpen up those images with Photoshop.  Even IF you can see the barcodes, can you read them?  Could a computer?  Be honest.<br>- Now, consider some of the "hyped" scenarios being put forth by folks at companies (like the guys on the google blogs).  Stand back from that window a moderate distance.  Look at that stuffed dog toy in the window.  Connect all the dots in the system you image.</p><p>Sure, "everything" is possible with enough assumptions and "a small matter of programming."  Hypothetically, standards and co-operation can solve lots of things.  But, when will these standards be here?  Why would they be created?  Who is paying, and why?</p><p>I do completely agree that this will happen.  The point of my post was that it's not as close as the demos and emerging hype would have "the common folk" think, and it is just this kind of "too soon hype" that the VR and AI communities engaged in.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2008/09/29/nintendo-ds-and-handheld-ar/</div>
<span id='oldComments'>
<h2>1 Archived Comment</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Tuesday, September 30, 2008, UTC, twDarkflame said:</span>
  <span class='text'><p>You nesscerly need to have props to have AR games.<br>A simple example would be one that turned the screen into a viewfinding, which you move around to shot at flying saucers overlaid onto the real-world view.<br>The enemys wouldnt be alligned to real-world objects, and if they are in the air, it would only need very apoximate tracking to where they can/cant fly.</p><p>Another possible game would be a varation on the Carcade. (<a href="http://www.youtube.com/watch?v=_Se2nhyetR8&amp;eurl=http://mark-logan.blogspot.com/2008/09/augmented-reality.html)" rel="nofollow noopener" title="http://www.youtube.com/watch?v=_Se2nhyetR8&amp;eurl=http://mark-logan.blogspot.com/2008/09/augmented-reality.html)">http://www.youtube.com/watc...</a></p><p>Any DS game would be along these lines I think, if at all.<br>I'm skeptical Nintendo would split the market of the DS by introduceing games that require a specific model.<br>I think more likely it will be the next gen of handhelds....which will probably have wiimote style sensors built in...that will be more suitable for AR.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2008/10/12/mobile-ar-oct12-08/</div>
<span id='oldComments'>
<h2>1 Archived Comment</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, October 13, 2008, UTC, twDarkflame said:</span>
  <span class='text'><p>Some day not long me thinks.<br>Theres going to be a tiping point, then it will be an arms race. (probably either by mobil phone or game companys)</p><p>I think the Model-based Tracking link showcase's a good path forward, particularly if data is collected on mass to create such models (or at least, key-points)  Microsofts Photosyth technology could well lead to a very large collection of landmarks around the earth automaticaly modeled in 3d from photos.</p><p>full angle sensing is now a low cheaper thanks to Nintendo pushing down the prices. (or at least, will be, when the millions or so orders of the small gyros needed for the wiimote upgrade go though).</p><p>And better GPS will be coming as soon as europe gets its act together with Gallio (accurate down to the meter for cilivian use).</p><p>Its only the output devices that are really lagging behind, imho.<br>Companys are still scared to go near anything remotely like VR headsets, even optical see-though or eye projection. (both very possible with todays technologys if they were invested into).</p><p>At the moment AR is either where a massive bulky custom headset (video-see-though), or lose the use of your arms.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2008/10/18/a-real-ar-book-not/</div>
<span id='oldComments'>
<h2>5 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, October 20, 2008, UTC, augmentedblog said:</span>
  <span class='text'><p>Dear Mr. MacIntyre, thank you very much for your partly constructive feedback. Maybe some additional information can help you to understand, why the first commercial application in this market domain still is breaking news for us.</p><p>What turns an invention into an innovation is market entry. And regarding the very conservative german publisher market (we even have a price fixing for books), it is a certain proof of concept, when two publishers decide to implement a quite new technology. So far, so what? You have to keep in mind, that the book "Aliens&amp;UFOs" is a children´s book, and as in many fairs before, our feedback at the Frankfurt Book Fair is slightly different to your experience, when showing it to potential users. The kids were fascinated. The interaction level is of course low, but even the intuitive possibility to hold 3D objects in their hand, made them expose to the content in a more substantial and entertaining way. 3D navigation seems simple to us, but there are some target groups, who get lost in zooming into three dimensional content rooms. From our point of view the real magic book (as a real product) was a FIRST success. And that´s what we need to develop AND implement (raise budget!) better interaction possibilities. Maybe the next step is touch interaction, maybe we are allowed to create the nonplusultra content, whatsoever. But it was very much effort to convince the first movers to try out something new. That a magic book is of course not something new to people from the AR world, no surprise. But if you leave the incubation of research and development, there are some surprises facing real markets. So please understand, that we were happy to substantiate our arguments for the relevance of this technology with real products in cooperation with really renowned brands. Nothing more, nothing less.</p><p>Kind regards, Jan.</p></span>
</li>
<li>
  <span class='source'> on Monday, October 20, 2008, UTC, blair said:</span>
  <span class='text'><p>Hi Jan,</p><p>I fully understand your point that bringing an actual real product to market is necessary, and important (especially for you, who are the makers of an AR technology).</p><p>I also understand that convincing a book maker to publish one is a big deal.  I have done consulting for "real" companies, and understand these issues.</p><p>And, I understand that you need to start somewhere, that the first product can't do everything.</p><p>You need to understand my viewpoint, however, that first impressions matter for new technologies hitting the marketplace.  I am not an ivory tower academic, complaining that some "commercial product" doesn't meet my standards.  I believe that the most important thing to ensure AR does well in the market is to have some of the first few high profile products be have good content and be compelling.  AR is the focus of much excitement and hype right now, and if the first few things that come out fall short of what people expect, that hype will pass and the market could shrivel and die without ever reaching its potential.</p><p>The fact that kids and visitors to a book show were excited means very little.  People are ALWAYS excited when they first see AR;  that's what I meant by the "gee wiz" factor.  The real question is "would they buy it, or more importantly, would they enjoy it, enough to recommend it to others?"</p><p>Perhaps the answer for your book is "yes".  I don't know.  I wish I could see the book, beyond the video of people using it, though.  I am not trying to be dismissive or overly negative about your product.  I'm sure it's pretty cool, and a fun thing for people to play with in a show/kiosk/etc context.  I also hope it does well, since it will be good for all of us!</p></span>
</li>
<li>
  <span class='source'> on Monday, October 20, 2008, UTC, mlogan said:</span>
  <span class='text'><p>I think the "so what" question is a great way to evaluate whether or not you're using technology for a purpose or just using technology. And I think it's a legitimate question about the video shown here.</p><p>However, I think that in this case, there is enough value merely in the "gee whiz" factor to make the case for using the technology. The subject matter of the book seems to lend itself to futuristic technology, so it's a great fit.</p><p>Beyond this particular subject, there is a fascination and novelty at the prospect of revealing something that can't otherwise be seen. Will that novelty wear off over time, sure. Readers and consumers will demand greater utility from future AR titles.</p><p>But it's a start, and an early indicator of momentum. I've been around for several technology adoption cycles now. This stage of early enthusiasm and primitive implementations that hint at future trends feels very familiar. Usually the technologies aren't implemented in the way the futurists expect, which often causes derision and premature claims that the hype is empty. Nevertheless, substantive change often materializes, but by the time it does, it seems so commonplace that it's hard to recognize as dramatic. It's only in those retrospective moments that we realize what a profound impact a given technology has wrought on our lives.</p></span>
</li>
<li>
  <span class='source'> on Saturday, November 22, 2008, UTC, blair said:</span>
  <span class='text'><p>On one hand, what you say is true;  obviously, getting the technology out there and generative interest is important.  In my own research, I often focus on building tools for non-techies to work with AR because I fully appreciate the idea that it won't be the researchers and technologists that generate the good ideas.</p><p>But, on the other hand, I still stand by my original comments.  There are technologies that fade away because of hype.  If people try out mediocre AR (ESPECIALLY if they pay for it) and realize there is nothing there, they won't pay a second time.</p><p>As a demo, that book is cool.  As a product, I don't think it looks like it is.</p></span>
</li>
<li>
  <span class='source'> on Friday, January 15, 2016, UTC, PlagiarismChecker said:</span>
  <span class='text'><p>Good day! I know this is kinda off topic however I'd figured I'd ask.<br>Would you be interested in exchanging links <br>or maybe guest authoring a blog post or vice-versa?<br>My site addresses a lot of the same subjects as yours and <br>I believe we could greatly benefit from each other.<br>If you happen to be interested feel free to send <br>me an e-mail. I look forward to hearing from you!<br>Awesome blog by the way!</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2008/11/12/nintendo-dsi-more-reports/</div>
<span id='oldComments'>
<h2>2 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Thursday, November 13, 2008, UTC, twDarkflame said:</span>
  <span class='text'><p>I think its likely it will do very limited AR.<br>It has already shown in videos functions such as replacing a colour on a live video stream. (or making everything black and white save for one colour).<br>So, while not 3D overlaying, it can alter a viewpoint of reality in realtime. So thats technicaly AR at least.</p><p>Given the current DS's specs, and assuming the DS is the same (which I think is basicaly the case), I think it should be also just about possible to display a basic 3D object on a well-markered card.<br>But I dought there be much processing left to do anything with it.<br>Maybe their will be supprise's when the homebrewers get hold of it.</p><p>Still, I think this is basicaly a bit expirement for Nintendo.<br>They are probably considering options right now for what their real next portable will be....what will be its main "hock".<br>And with any luck that will be AR games.</p></span>
</li>
<li>
  <span class='source'> on Thursday, November 13, 2008, UTC, blair said:</span>
  <span class='text'><p>Yes, I agree;  I think that it is promising and will hopefully point to the future of handhelds as AR gaming platforms.  I am still sad that the PSP camera never amounted to much, but since we can't actually program the PSP that's not such a big deal (I like the PSP camera, btw, the image quality is great!)</p><p>That said, while you could classify the image processing hacks in that video as "AR", I think it's stretching it to do so.  Doing such a mod to a video stream is a HUGE step away from doing any more "interesting".  It's AR in the same sense as the PS2/PS3 Eyetoy games (that use the video of you from motion control) are AR ... they are changing reality, but not in any particularly interesting way.</p><p>Heck, if that's AR, then most current video chat programs are AR since they allow you to apply image effects to the video on the way out. :)</p><p>I will be interested to see what the homebrew folks do, but I'm much more interested in the next gen platform hardware at this point (e.g., things based on TI OMAP3 or NVidia Tegra).</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2008/12/01/a-little-iphone-ar-demo/</div>
<span id='oldComments'>
<h2>6 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Thursday, May 27, 2010, UTC, nlw0 said:</span>
  <span class='text'><p>That is really very cool, congratulations!...</p><p>I just wanted to say that we should not ignore the work involved in hacking the iPhone. Developers should think a little more about themselves and look for the most open platforms, where it's easier to work. Don't force yourself to use the iPhone. Let Apple make it more attractive for this kind of application if they care about us.</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Thursday, May 27, 2010, UTC, blair said:</span>
  <span class='text'><p>Right now, the iPhone is the only platform with a sizable market, so while I sympathize with your sentiment, it's not realistic at this point.</p><p>That said, we're hoping to switch our research over to Android this summer, pending some imminent developments.  I hope that we'll be using that much more open platform for our research starting in a few weeks!</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Tuesday, August 7, 2012, UTC, Mrunal said:</span>
  <span class='text'><p>Can you please guide me how to start coding into AR development in iOS technology?</p><p>Which API/framework is useful for that?<br>Is there  any documentation available then it would be helpful.</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Friday, August 10, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>I would use Qualcomm's Vuforia SDK.  Either do native coding (if that's something you are comfortable with) or use Unity3D.  Also, you could use our <a href="http://argon.gatech.edu" rel="nofollow noopener" title="http://argon.gatech.edu">Argon browser</a> (the next version with have Vuforia in it!)</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Sunday, August 11, 2013, UTC, Nitin said:</span>
  <span class='text'><p>Even though I hate the idea of virtual pets, I like the concept at play here.</p></span>
</li>
<li>
  <span class='source'> on Saturday, February 20, 2016, UTC, paykasa bozdurma yüksek kur said:</span>
  <span class='text'><p>thank you very much for this post <a href="http://www.bozdurma.org/" rel="nofollow noopener" title="http://www.bozdurma.org/">http://www.bozdurma.org/</a></p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2009/01/19/interview-about-a-new-ar-company/</div>
<span id='oldComments'>
<h2>2 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, February 9, 2009, UTC, robertrice said:</span>
  <span class='text'><p>Blair,</p><p>Thanks for the mention. The interview grew out of a conversation I had with Tish Shute about myself, the company, and what our vision for the future is. I would like to clarify that I was not trying to dismiss putting graphics in the real world...but rather that AR shouldn't be limited to "graphics on top of video". I'm not trying to redefine "perfectly good definitions" for my own purposes, rather, I'm trying to encourage discussion about it because I don't think we have things defined well enough...we keep borrowing terms from virtual reality, virtual worlds, MMOs, social media, etc. You can't describe, discuss, or collaborate on something without a common ground of language.</p><p>Anyway, thank you again for the mention. I share your excitement and enthusiasm for the tech and the developing industry. I'm looking forward to seeing the fruits of your work and research.</p><p>Robert Rice</p></span>
</li>
<li>
  <span class='source'> on Tuesday, February 10, 2009, UTC, blair said:</span>
  <span class='text'><p>I understand what you are saying here.</p><p>However, given that AR has had a perfectly good and accepted definition for about 20 years, and that definition involves registering graphics (or other media, such as audio or text) with a view of the world (not just "on video", since most of us who've been doing it for years are big fans of optically transparent displays), trying to expand that definition to include other things is, by definition, "redefining it". :)</p><p>"Mixed reality" is a more expansive term, that includes AR and other "location based content" ... using that for the broader set of approaches that mix physical and virtual worlds would fit better with how the terms have been used for a while.</p><p>As you say, we can't really discuss things without a common language.  Since the terms are already there, it would makes sense to just use them.</p><p>The reason I bother to make this point is that people are excited about "augmented reality" because of the demos and concepts and prototypes that do "put graphics on top of video" ... trying to appropriate that excitement for a broader class of things does exactly the opposite of what you want, which is to create clear definitions and common language.</p><p>I think there are lots of cool things happening in AR, MR, ARGs, VR, MMOs, and so on ... but if everyone just picks their favorite term to describe whatever it is they do, we won't ever have common understanding!</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2009/03/09/topps-and-total-immersion-miss-the-boat/</div>
<span id='oldComments'>
<h2>1 Archived Comment</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Saturday, May 2, 2009, UTC, freedimensional said:</span>
  <span class='text'><p>I am releasing a series of blog posts about AR that you might be interested in - <a href="http://bit.ly/B0VOc" rel="nofollow noopener" title="http://bit.ly/B0VOc">http://bit.ly/B0VOc</a> - would love your feedback</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2009/09/23/has-ar-taken-off/</div>
<span id='oldComments'>
<h2>4 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Friday, October 2, 2009, UTC, moombe said:</span>
  <span class='text'><p>What I don't get is that in the interview of Bruno Uzzan (Total Immersion), he seems to say he was confident to release soon AR apps for mobile devices that would solve the near field recognition problem. Mentionning, if I'm not mistaken, the iPhone. Knowing the current iPhone SDK does not allow to analyze a live video feed, I simply do not understand how he can claim his words...<br>Anyway, on a jailbroken iPhone, you can do it. I've seen a few demos of AR-marker based applications which seems quite nice. So we need to have apple understand they have to open a bit more the SDK.</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Friday, October 2, 2009, UTC, blair said:</span>
  <span class='text'><p>I just chatted briefly with Bruno yesterday at the NVidia GPU Tech conference about this very thing;  both his company (Total Immersion) and Metaio (the CEO, Thomas Alt, was here speaking, too) have markerless tracking solutions in development on the iPhone, and both showed videos of it working.  I asked both when it would be available; both pointed out that they have it working, using video access hacks (similar to the hacks my group and others use), but can't release it till Apple opens the API.</p><p>More interesting to me, both said they didn't even know when it would be available to developers (who, like me, would want to be able to work with it in anticipation of it being available).</p><p>Regardless of that fact that it's not yet available, I think it's huge news that there are now 4 different NFT demonstrations on the iphone.  These two commercial ones, along with research systems shown by Georg Klein and Daniel Wagner.   Very very exciting.</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Monday, January 18, 2016, UTC, best bay st louis real estate  said:</span>
  <span class='text'><p>Thanks for sharing such a pleasant idea, post is nice, thats why i have read it entirely</p></span>
</li>
<li>
  <span class='source'> on Thursday, April 7, 2016, UTC, nick1538 said:</span>
  <span class='text'><p>Nice<br>to be visiting your blog again, it has been months for<br>me. Well, this article that I've been waiting for so long. I need this article<br>to complete my assignment in the college, and it has same topic with your<br>article. Thanks, great share.</p><p><a href="”" rel="nofollow noopener" title="”">Android App Developer</a></p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2009/11/29/art-hack-nikon-projector-camera/</div>
<span id='oldComments'>
<h2>4 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Wednesday, December 2, 2009, UTC, elvis_zheng said:</span>
  <span class='text'><p>How do you think about SixthSense?<br>It uses wearing projectors as output device for Wearable AR, and well demonstrates its pratical usefulness.</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Wednesday, December 2, 2009, UTC, blair said:</span>
  <span class='text'><p>I'm aware of SixthSense, yes.  It's a cute grad student hack, with some well put together demos.  But, I don't see it as ever working in the way they present it, and I've admit to being a bit surprised that so many people are buying into it.  During questions after her ISMAR talk, Pattie admitted that the none of the presented scenarios actually work, they are all just demos/envisionments.   Yet, at the same time, she was also unaware of the other work that's been done in this space, especially in Japan, where people use shoulder/head/body mounted projectors (or, in the case of tele-robotics, robot-mounted projectors) to do some of the same things that they present in their videos; of course, those other systems are more limited, but they also seem to work for what they do.</p><p>In general, I don't see this "worn projector" setup as working.  Hang a projector (or flashlight) around your neck;  now, go and pretend to do useful things using the image it projects.  The image will move around unless you are very still;  if you project on any non-trivial surface, the image will by uneven and hard to interact with or perceive;  it won't point in the direction you'd like it to point unless you contort your body;  lean back, look around, sit comfortably, and it won't be projecting anywhere useful.</p><p>I think the work of Ramesh Raskar and Oliver Bimber, who do projective AR but in more controlled and well constrained situations, is much more on the mark, and far more compelling.</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Tuesday, December 15, 2009, UTC, elvis_zheng said:</span>
  <span class='text'><p>Thanks for your detailed explanation. I see your points why you think "worn projectors" are not that useful.</p><p>I have thought about why "so many people are buying into it" for a long time. Maybe because the SixthSense satisfied people's expectations of mobile social interaction to some extent and here the Internet was made alive or "Outernet". From this point, I think social AR or AR 2.0 will attract more attention.</p></span>
</li>
<li>
  <span class='source'> on Wednesday, January 13, 2016, UTC, Paul said:</span>
  <span class='text'><p>I am fairly certain the 3 telephone does, but I think it does it at the 3 machine but may be started throughout the telephone that is soft.</p><p><a href="http://Www.axxpac.de" rel="nofollow noopener" title="http://Www.axxpac.de">http://Www.axxpac.de</a>, <a href="http://www.Hks-Dc.org/find-out-the-free-video-editing-software-for-windows/" rel="nofollow noopener" title="http://www.Hks-Dc.org/find-out-the-free-video-editing-software-for-windows/">http://www.Hks-Dc.org/find-...</a>, <a href="http://Www.Fixingwindows8.com/how-to-record-a-video-interview-with-skype-by-ecamm-call-recorder/" rel="nofollow noopener" title="http://Www.Fixingwindows8.com/how-to-record-a-video-interview-with-skype-by-ecamm-call-recorder/">http://Www.Fixingwindows8.c...</a>, <a href="http://Www.Hks-dc.org/find-out-the-free-video-editing-software-for-windows/" rel="nofollow noopener" title="http://Www.Hks-dc.org/find-out-the-free-video-editing-software-for-windows/">http://Www.Hks-dc.org/find-...</a> I personally do <br>not utilize phone recording but from the it being advertised <br>as an attribute.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2009/12/17/aura-interactive-launches-santavision/</div>
<span id='oldComments'>
<h2>2 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Tuesday, December 22, 2009, UTC, dylski said:</span>
  <span class='text'><p>Sounds like a great concept. Is there going to be a video demo of it for us unfortunate enough to lack iPhones?</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Thursday, December 24, 2009, UTC, blair said:</span>
  <span class='text'><p>Here you go!   <a href="http://www.youtube.com/watch?v=5StDJkKJEGM" rel="nofollow noopener" title="http://www.youtube.com/watch?v=5StDJkKJEGM">http://www.youtube.com/watc...</a></p></span>
</li>
</ul>
</li>
</ul>
</span>

<div class='commentURL'>/2010/01/05/jarrells-state-of-the-art/</div>
<span id='oldComments'>
<h2>3 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Sunday, January 10, 2010, UTC, Feng Zheng said:</span>
  <span class='text'><p>From this post and Jarrell Pair's summary, I learnt a lot again, and grasped where the AR is and which possible directions AR is heading to.</p><p>What about your research of using crowdsourced images to solve the problem of positioning instead of with "GPS+Compass"?</p><p>Maybe there is another way to this problem or as an auxiliary measure, I think, which is more traditional, but may be laborious. That is, placing markers into the real world, places of interest/importance for positioning. We may use Bokode developed at MIT Media Lab, which is very small as "Imperceptible Visual Tags for Camera Based Interaction from a Distance". How do you think about it?</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, January 11, 2010, UTC, blair said:</span>
  <span class='text'><p>Hi Feng.   I don't personally do the crowdsourcing image work for tracking;  it's just one of those ideas that the folks who do this kind of work agree is good.  One of my students is collaborating with folks at Nokia research on a similar idea, for example.</p><p>I think the Bokode's are a very cool piece of technology;  using them might help, although it's unclear what they would really give you in terms of tracking, at least outdoors (GPS gives you crude location to start with).  Indoors they might help, of course, but so could other passive approaches.</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Friday, January 22, 2016, UTC, http://www.amigosdanatureza.ne said:</span>
  <span class='text'><p>We're a group of volunteers and opening a new scheme <br>in our community. Your site provided us with helpful information to <br>work on. You've performed a formidable activity and our entire <br>neighborhood might be grateful to you.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2010/01/15/game-education-lipstick-on-a-pig/</div>
<span id='oldComments'>
<h2>1 Archived Comment</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Tuesday, January 26, 2010, UTC, Feng Zheng said:</span>
  <span class='text'><p>As a student, I think it is helpful to have a game major, because some students are really into developing games. In China, as far as I know, no universities offer game major. The closest may be my major -- Software Engineering (Digital Media) in Software Department. Many courses in my department, if one choose, are taught by staff in game companies, and these courses are popular. However, I agree with your opinion, if a student wants to learn more about CS, the courses offered by the game major may be too narrow.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2010/04/30/clever-use-of-ar-for-public-safety-awareness/</div>
<span id='oldComments'>
<h2>1 Archived Comment</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Sunday, January 17, 2016, UTC, homes for sale bay st louis said:</span>
  <span class='text'><p>When I originally commented I clicked the "Notify me when new comments are added" checkbox <br>and now each time a comment is added I get several emails <br>with the same comment. Is there any way you can remove me from that service?<br>Thanks!</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2010/05/01/how-will-people-play-augmented-board-or-card-games/</div>
<span id='oldComments'>
<h2>3 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Saturday, July 7, 2012, UTC, The Board Gamers said:</span>
  <span class='text'><p>It would be cool if someone could somehow implement a way of offering an online board game player with a way to cheat (without them knowing and without being obvious) and then counting how many times they do so.</p><p>Like you said, it's an interesting scenario because while the game is the same in both scenarios there's always going to be a psychological difference.</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Tuesday, July 24, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>Yes, it would be interesting;  I've tried playing Catan online and it's just not as much fun (same with Magic).  I think that the many elements of tension that real world interaction brings are hard to match online.</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Friday, January 15, 2016, UTC, PlagiarismCheckerOnline said:</span>
  <span class='text'><p>Whoa! This blog looks exactly like my old one!<br>It's on a totally different topic but it has pretty much the same layout and design. <br>Excellent choice of colors!</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2012/04/05/185/</div>
<span id='oldComments'>
<h2>14 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Thursday, April 5, 2012, UTC, Jez said:</span>
  <span class='text'><p>Couple of issues with your article.</p><p>First of all it's pretty clear that this is a concept video -  the title of the video is 'Project Glass: One Day...' which is obviously supposed to be read both ways, suggesting that this is a vision for the project and maybe not the final product itself. Plus it seems that the video was launched by google as part of a consultation stage for project glass so is designed to introduce people to AR and inspire thought and comment rather than as a product launch vid.</p><p>I agree that the first gen of this system will be significantly worse than what you're seeing in the video but I think it's much more important to give people an idea of the basic potential of AR - they've obviously reigned themselves in massively from exploring the more exciting scope of this sort of system.</p><p>Otherwise, your stability issue seems iffy. Most if not all of the overlays aren't being locked to objects in vision so there's no reason to think that as long as they stay in a stable position on the screen they wouldn't appear stable to a user. Just look at a scratch on your sunglasses.</p><p>Your depth of field issue is wrong as you can clearly see in the video that only either the overlays on the screen or the normal vision are in focus at any one time. The actual experience may be more extreme but they've certainly not missed it out.</p><p>I think your HUD vs AR differentiation is unnecessary - you may have basic overlays to start of with but this will quickly develop into a much more immersive experience.</p><p>I can see why you might take issue with the video but only if you're seeing it as some sort of launch video for the tech and not what it seemingly is - a bit of buzz creating fun.</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Friday, April 6, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>Hi Jez, thanks for the comments.</p><p>Yes, it's obviously a concept video; it's just poorly done.  Given the context of the video, the project and the group it's coming out of, this is not something they just whipped up last week.  Given the brainpower of the team behind it, from who we all know great things can come, I don't see any reason to assume they didn't create exactly the video they wanted to create.</p><p>If the goal is to inspire thought and comments, they should have been more careful with their video design.  They are presenting a direction they can't achieve.  This is not a marketing commercial (remember the Microsoft book, and the Nokia glasses, both from a few years ago);  this is a video from Googles super-smart-super-secret labs, the folks that brought us the self-driving car.</p><p>Nothing in this video is "AR", btw.  It's a simple heads-up display.  They simultaneously show some nice, well designed hardware (prototypes?  mockups?) that have a display off to the side, and then show a video that covers the entire field of view of the user with 2D (not 3D AR) content.  They create the idea that they think it's a good idea to completely cover someones view of the world with a mostly opaque map.  Really?</p><p>BTW, the step from 2D HUD to AR is neither inevitable nor quick;  the technology requirements are very different.  The hardware mockups are well designed for an unobtrusive HUD, not for AR.</p><p>And I think the terminology difference matters.  It's impossible to have a discussion about something if people use the same words for different concepts!</p><p>I don't think it's a launch video, and I think it's creating the wrong kind of buzz and expectation.  Perhaps I'm just too close to the subject matter.  I don't think that most people will take this as a totally fake concept video, regardless of what they say.  They've clearly put a lot of time and thought into this;  I wish they'd made a better video that would match a realistic vision of what would be possible in the next few years.  They certainly could have!</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Friday, April 6, 2012, UTC, Jez said:</span>
  <span class='text'><p>Hi Blair, thanks for getting back to me.</p><p>I'm no expert in this area by any means so please correct me if I'm wrong but, apart from the field of vision issues with the video - compared to the first gen glasses that are being currently developed - I can't really see any elements of the video that are crazily far out there in terms of what could feasibly come out in the next few years. Most can be achieved with the tech that comes in mobiles now, gyros, gps etc. and wouldn't need heavy video processing. Give it a couple of generations, faster connectivity and I can't see why any of this is impossible. Not sure where we are in terms of full field of vision displays so if we're way off on them then that could slow the adoption and development of the area. Assuming adoption rates are reasonable though, wouldn't rates of hardware development to suit this sort of device's requirement increase making this and much more potentially attainable? Seems like this area could also do wonders for the field of robot vision.</p><p>Why is there necessarily a line between a HUD and AR - when does one become the other? Don't you consider the overlay of a map onto a scene to be augmenting reality? When someone develops a blue sky app that detects the sky and replaces it with one of your choice? Seems like a foggy, gradual but pretty natural transition rather than a sharp distinction. What's holding back that transition if, as you say, it's uncertain? I would have thought that given the addition of a few more gizmos, IR? depth perception, really good orientation detection and pretty shit hot vision processing that you might get on the Gglasses 4 you're getting close to something really cool in terms of capability.</p><p>Also, you'd kind of assume that those full overlays are at least controlled in some way rather than popping up unbidden. Assuming they are, why is that any more weird than looking down at your phone for a second to check where you are on the map?</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Saturday, April 7, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>Much of what is in the video is possible, I would agree:  when it's situated on a little display up and to the left of your eye, so you can glance at it when you notice it changing, it would even be useful.  I talked to various reporters who picked up the story, and some tried to relate that.  The issue I have is that the video doesn't portray it as "off to the side", and creates a very fake sense of the experience.  Worse, they carefully align 2D stuff with the relevant bits of the world to make it appear that the little annotations are linked to the world.</p><p>Going from small (~15 degree) to large (90 degree or more) FOV, in a wearable, sleek package will be very hard. People have been trying for more years than I've been doing research (Sutherland built the first see-through HMD back in the 1960's, around when I was born!)</p><p>The line between AR and HUD is pretty simple.  AR implies content aligned with the user's perception of the world;  a HUD is 2D info (perhaps related to the world around you, just as 2D stuff on your phone might be related to the world around you).  If it's not AR, you don't even need a see-through display, and it may be worse to have one (from a usability perspective) because of the false impression of alignment, and because the content will be harder to read (contrast, cluster, visual confusion, etc).  Overlaying a map on the scene, when the map is just floating in a fixed location relative to the display is not AR;  modifying the sky is (and would be cool, especially if you synchronized the color to your music ;)</p><p>More gizmos == more bulk and more complexity.  You won't be getting a Kinect into an HMD any time soon.</p><p>Don't take me for a pessimist;  I fully expect to see real AR HMDs before I retire, and look forward to using them.  I like the physical construction of the Glasses, and think I would actually use them (if I can code for them).  They aren't AR, but neither is my purely mechanical watch ... and I like it and find it quite useful.</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Friday, April 6, 2012, UTC, Ross Graeber said:</span>
  <span class='text'><p>I've been mulling this over as well.  They could create some fantastic ambient notifiers with a HUD plus audio.  Subtle color changes or motion in the periphery could create both calm and active alerts for a user to adjust their focus when they want.</p><p>But your point remains, it won't look like this if it's a traditional LCD or LED HUD projected on some optics.  We can't look two places at once.  So this whole project video is as annoying as it is exciting.</p><p>At the same time there's some intriguing patents at Google around Virtual Retina Displays (VRD):</p><p><a href="http://www.google.com/patents/US5659327" rel="nofollow noopener" title="http://www.google.com/patents/US5659327">http://www.google.com/paten...</a></p><p>Do you think if they were hiding a VRD behind their ear it could be a different picture?</p><p>Though, I'd think a consumer priced, tested, portable, and viable VRD might be me starting the rumor mill off with no gas in it.</p><p>Would make for some fun ads poking at Apple's Retina Display, though.</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Saturday, April 7, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>Would be really interesting if they had a VRD in there.  It still wouldn't look like the video, I don't think, but it would solve some of the ambient light issues, etc.</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Friday, April 13, 2012, UTC, Nicolas said:</span>
  <span class='text'><p>Brother already relesed a "Virtual Retinal Display". They call it "Airscouter". Apparently it is already possible do do the things Google showed in their video two years ago. The Brother glasses have a 800x600 resolution and you don't need to focus on the projected image. The projected screen is equal to a 16" screen in 1 distance meter. Also the size of their device isn't much bigger than Google Glass.</p><p>Maybe in another two years it is a 20" virtual screen with HD resolution...</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Wednesday, April 18, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>Yes, others have been developing VRDs; although, I wouldn't say Brother's has been "released" (Engadget <a href="http://www.engadget.com/2012/04/17/brother-airscouter-glasses-bring-augmented-reality-unsightly-ad/" rel="nofollow noopener" title="http://www.engadget.com/2012/04/17/brother-airscouter-glasses-bring-augmented-reality-unsightly-ad/">just posted an article this morning</a> about them finally releasing them in Japan this summer).  Like google's, the Airscouter is a really small image (16" at 1 meter is really small);  there's a reason these companies never say what the effective field of view of their displays are, after all, but prefer to quote weird sizes that tend to mislead folks into thinking they are bigger than they are ("It's like a 100" display, from 20' away").  I'd also disagree with your characterization that they aren't much bigger than Google's prototype, they are huge!  (Check out the picture in the engadget article above).</p><p>That's all besides the point.  My comment about Google's project can be summarized this way:  their display wouldn't cover your vision with virtual content as their video implied, and the interface they present isn't really appropriate for display in the center of the user's visual field.  The Airscouter could put content in front of one eye, but if you were going to design an interface for a display that blocks your vision, you wouldn't want the interface in the Google video.</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Tuesday, July 3, 2012, UTC, Michael L. Croswell said:</span>
  <span class='text'><p>My first reaction to the video was the same as yours.   A bunch of us (US Navy contractors) did a stereo HUD see-through AR project in 1993.  We had tracked a maintenance object with CV in 3D and did corresponding 3D reticles.  The issues that you discuss FOV, light-dimming, focus (even auto-focus based on tracking vergence) are very important for this kind of decent AR see-through system.  Blair, I'm very glad you brought these points up!</p><p>So, to those that don't know:  Augment Reality means many things to many people.  To the original researchers it meant that the display is augmented onto the world, for some it was just extra information for others it is more - a one-to-one mapping of the real world (in the background) with 3D-aligned imagery that fits the real world like a glove (with appropriate holes and transparency).    This is the aspect that could/should have been shown in the video - like a wire-frame model of the Moscone Center with a wire-frame landing-zone super-imposed on the actual one.  That would have been AR!</p><p>Your and the others discussion in the comments on VRD is interesting too.  You probably know about Microvision's earlier work on that, yes?</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Tuesday, July 24, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>I have a Microvision Expert Technician display in the lab! :)</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Thursday, July 26, 2012, UTC, James Michaels said:</span>
  <span class='text'><p>Interesting post, Blair. It took me a little while to envisage what you are suggesting, but I get your point now.</p><p>What interests me about this project, though, is not so much that this is most certainly not going to be an AR project, which doesn't bother me at all - I am happy for the final product (version 1.0) to be a 2D HUD display - but the fact that they choose to use that tiny little display just above the eye. Wouldn't it make more sense to just build a larger display that covers the whole eye? I mean that, for me, (not the camera, etc.) is what I would like as a consumer. I'm surprised that they can build a 2cm x 2cm display yet not create one the size of a standard glass lens. (Perhaps I don't understand the dynamics here. Does this somehow project onto the eye creating a larger image?)</p><p>My impression, and please jump in anytime to correct me here, is that the finalized display will be about a thumb sized viewing area over top of the eye. This seems like it would be pointless for a large number of different activities: The main ones being viewing the web or playing games, which is what most people buy a smart phone for in my opinion. Also, viewing YouTube at that size or watching a movie with subtitles (or the translation app that is being touted so highly on the internet right now) would be difficult. There is also the element of eyestrain. Wouldn't the design of the display require the user to be constantly looking upwards when using it? That doesn't sound good for one's eyes.</p><p>Basically, I am rather excited by the potential, but all I see from Google is a camera that one wears on their head that has various features similar to a smartphone, yet a screen too small to be usable. As a consumer and a lover of technology, I am highly saddened by that concept, but at the same time I respect that this is not a simple creation. There is a ton of R&amp;D going on and Google is obviously trying to make this a workable project. That said, as the display cannot live up to any reasonable expectations I might have of a smartphone like device (such as internet browsing, gaming, even messaging in long form or email), it is hard to not feel that this will go down as a product failure for Google.</p><p>(I am also quite fascinated by the concept of a VRD - thank you for the education folks. I have my doubts that they could miniaturize the technologies; however, the device as it stands looks like it could accommodate a small projector above the eye for this purpose. The lens itself could be a temporary measure to avoid showing their trump card - or not.)</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Saturday, July 28, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>Hi James.  I agree with you;  I think it will be very useful as a 2D HUD.  I am excited to see how that succeeds, even though its "no AR".</p><p>Regarding the display:  I think they actually chose the right size, for two reasons.  First, the optics required to build a larger display, and especially one that wouldn't cause weird visual artifacts when viewed through on a regular basis, aren't really "there yet".  Bigger would be bulkier, and less comfortable.  And would, thus, be the kiss of death for acceptance.  Second, regardless of the quality, I think the safety issues surrounding a display that covers your eyes are immense.  If you put it on/took it off when you wanted to use it, then covering your visual field would be fine.  But, they want to build something that people will leave on all the time.  And to do that, you either need to be able to sense an incredible amount about the world around you (to ensure you don't block things people need to see, etc), or you need to sit it off the side, out of the way.  The former isn't possible, so the only practical option is the later.</p><p>The use case for these displays are not anything you use you phone for right now.  It's not about playing games, or watching videos.  It's about the system constantly displaying micro-bits of might-be-useful information that is based on you location, activity, who's near you, what's going on, and so forth.  It's about glance-able content, not "deep" content.</p><p>I expect over time that the system will pair with your phone, and become a secondary display to the main phone;  I don't believe they can build any reasonable form on interaction with it's the way they are trying to do, and the phone (with it's touch screen) will be a beautiful complement to it.</p><p>But, regardless of where it goes in the future, the Glasses are about entirely new forms of interaction, and new kinds of applications.  They aren't about moving what you do on your phone or laptop onto the display (except for those things that currently suck on the phone, and would be better up there! :)</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Saturday, July 28, 2012, UTC, James Michaels said:</span>
  <span class='text'><p>Thank you. I really hadn't thought aboutit that way and you've really helped me to see how this might work in reality. Good post and comment!</p></span>
</li>
<li>
  <span class='source'> on Wednesday, November 21, 2012, UTC, Marc Z said:</span>
  <span class='text'><p>I'm not sure this type of technology can be that successful.  Yes it would be cool because its new but compare this to this movie industry.  People still go to movies all the time because of the entertainment atmosphere.  We have big 60" TVs at home and some even better theater rooms, but we still want that atmosphere.  I would love to test the Google Glass out though.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2012/04/18/finally-an-ar-translation-app-and-its-not-on-ios/</div>
<span id='oldComments'>
<h2>2 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Tuesday, April 24, 2012, UTC,  said:</span>
  <span class='text'></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Tuesday, April 24, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>Well, perhaps not mail (WP7.5 seems pretty good with both gmail and IMAP now), but I haven't seen a way to get caldav (a pretty common standard) to work for my cal.  I can't see how to get any non-exchange/AS cal to work.  Would be pleased to be wrong: I've been playing with a WP for the past few days and I really like it, otherwise!</p></span>
</li>
</ul>
</li>
</ul>
</span>

<div class='commentURL'>/2012/09/17/the-achilles-heel-of-project-glass-the-camera/</div>
<span id='oldComments'>
<h2>2 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Tuesday, September 18, 2012, UTC, Eran said:</span>
  <span class='text'><p>On a related note, you should watch episode 3 of the "Black Mirror" mini-series if you haven't already.  It deals with some of the possible future implications of this and related technologies.</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Tuesday, September 18, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>Haven't seen it, but read the plot summary:  pretty much what the typical sci-fi fear is with continuous capture.  We hashed this out a lot at GT years ago, both w.r.t. the wearables folks and the potential for continuous "life logging" and the possibility for continuous capture in the Aware Home.  Ironically, the hard part is access (how do you find the interesting bits);  of course, the technical folks often focused on that as a goal, rather than stepping back and saying "do we want that."</p><p>When someone proposed a continuous audio capture in the home, some of us dubbed it the "Divorce Accelerator" for even simpler reasons than that put forth in that episode.  Daily life would get ugly fast: "No, you didn't say that, you SAID ..."</p></span>
</li>
</ul>
</li>
</ul>
</span>

<div class='commentURL'>/2014/03/28/oculus-facebook-and-augmented-reality/</div>
<span id='oldComments'>
<h2>1 Archived Comment</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Saturday, January 16, 2016, UTC, plastic extruder said:</span>
  <span class='text'><p>Because the good pellet progresses the item melts as a consequence of <br>shear from the wall membrane and for that reason touches and moves to the liquid sales channel.<br>* Ram Extruder. The screw must also be designed to suit a specific type of <br>die.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2009/12/02/zemoga-launches-augmented-reality-practice/</div>
<span id='oldComments'>
<h2>3 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Wednesday, December 2, 2009, UTC, elvis_zheng said:</span>
  <span class='text'><p>It's good to see more and more companies launch augmented reality practice, to make AR commercialize. <br>But I wonder when AR will be commonplace? 5 or 10 years?<br>I think it may depend on when the fundamental problems are solved, such as robust and efficient natural feature tracking, especially on mobile devices, display devices, etc.</p></span>
</li>
<li>
  <span class='source'> on Tuesday, January 5, 2016, UTC, Ainara said:</span>
  <span class='text'><p>como mola el juego me lo cogi barato en este sitio</p></span>
</li>
<li>
  <span class='source'> on Tuesday, January 12, 2016, UTC, best bay st louis real estate  said:</span>
  <span class='text'><p>Hi, i read your blog from time to time and i own a similar one and i was just <br>wondering if you get a lot of spam responses? If so how do you protect against <br>it, any plugin or anything you can advise?<br>I get so much lately it's driving me crazy so any support is very much <br>appreciated.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2014/09/14/we-need-new-business-models-in-iod/</div>
<span id='oldComments'>
<h2>9 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Tuesday, September 16, 2014, UTC, Mark Wilcox said:</span>
  <span class='text'><p>Blair,</p><p>Have you seen IBM &amp; Samsung's recently proposed decentralised IoT system based on Bitcoin, BitTorrent and Telehash? They will supposedly be presenting it at CES in January.</p><p>Here's a link to their PDF: <a href="http://public.dhe.ibm.com/common/ssi/ecm/en/gbe03620usen/GBE03620USEN.PDF" rel="nofollow noopener" title="http://public.dhe.ibm.com/common/ssi/ecm/en/gbe03620usen/GBE03620USEN.PDF">http://public.dhe.ibm.com/c...</a></p><p>Mark</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Tuesday, September 16, 2014, UTC, blair said:</span>
  <span class='text'><p>I hadn't heard of it, I'll check it out.  Thanks!</p></span>
</li>
<li>
  <span class='source'> on Tuesday, September 16, 2014, UTC, blair said:</span>
  <span class='text'><p>Mark, I read it.  Interesting.  Definitely aiming in the same direction I was suggesting, and is an interesting proposal to use "blockchains" from Bitcoin/etc as a method to support decentralized, anonymous trust.  I'm not up enough on the technical side of things to know if this is a viable proposal (i.e., what are the secondary effects that are not being accounted for, how will this go wrong, etc), but it seems really interesting!</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Sunday, October 5, 2014, UTC, Joseph S said:</span>
  <span class='text'><p>Look forward to hearing your thoughts on this - Paul Dourish in his latest book, thinks that privacy comes with far too much cultural, ideological baggage...</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, October 6, 2014, UTC, blair said:</span>
  <span class='text'><p>Hi Joseph;  please say more:  do you mean we have too much cultural baggage when interpreting and so we should chill out, or we need to consider cultural baggage when interpreting?   (Sorry, I haven't read Paul's book, there are too many things to read and too few hours in the day ... it's on a list of things to read, but so far down in the queue it'll likely never pop out! :)</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, October 6, 2014, UTC, Joseph S said:</span>
  <span class='text'><p>Paul's book Divining a Digital Future. Yes, he takes the former view. However, he overlooks a point that Luciano Floridi makes, namely that in the infosphere, the affordances and designs essentially evolve into technologies of the self: <a href="http://www.philosophyofinformation.net/publications/pdf/tinopi.pdf" rel="nofollow noopener" title="http://www.philosophyofinformation.net/publications/pdf/tinopi.pdf">http://www.philosophyofinfo...</a><br>That said - I hope we get to hear more about your Keynote and IoT (and how you think we should address regulatory challenges ;-</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, October 6, 2014, UTC, blair said:</span>
  <span class='text'><p>I would agree that we need to step back and reconsider privacy as it's discussed in the tech world:  there seem to be two sides, the two extremes of which say "Privacy is dead, 'kids these days' don't care about their privacy" and "We need to lock everything down so you can guarantee privacy of your digital data."  I'm not sure what Paul and Genevieve say, but any discussion is good, so I will try to find time to read it (pointers to a specific part of the book would be good).</p><p>On the essay;  sure, ICT's become tools of self.  It was an interesting essay, but doesn't add that much to the discussion w.r.t. privacy (for me).</p><p>For regulatory challenges, I suspect looking at the writings and work of my colleague Peter Swire (<a href="http://peterswire.net" rel="nofollow noopener" title="http://peterswire.net">http://peterswire.net</a>), with whom I'm teaching a class in the spring, will be much more illuminating that what I have to say.  :)</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, October 6, 2014, UTC, Joseph S said:</span>
  <span class='text'><p>Agree - Floridi, an interesting frame. Lawyers, such as Peter Swire, would not be averse to the concept of personhood and its relevance to privacy/autonomy. Dourish et al. at p. 157 suggest that the term "privacy" "obscures rather than illuminates". They acknowledge the contingency of contexts - and suggest that we ought to shift discourse from privacy to accountability (p. 159). Is this another form of Helen Nissenbaum's "contextual integrity"? <br>Interesting - that Dourish et start by referencing Weiser's Mark Weiser's 1991. How prescient - a decade later we have the classic iPod and now IoT.</p></span>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
  <span class='source'> on Tuesday, January 12, 2016, UTC, my google account login said:</span>
  <span class='text'><p>Activate the tuned in to view information regarding your nearby <br>match if you discover the profile suitable, catch up <br>in seconds. Google gave life to some rumor last week with all the launch of your new social networking product as.<br>SEO is a sure bet in generating qualified engaged people to your website.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2012/03/14/what-you-want-us-to-be-your-dancing-bears/</div>
<span id='oldComments'>
<h2>15 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Wednesday, March 14, 2012, UTC, Kristina Höök said:</span>
  <span class='text'><p>I totally agree! And this is why CHI was changed this year, so that we would have two kinds of demos: <br>- "only" demo <br>- paper + demo<br>I think that the Interactivity chairs worked really hard on raising the level of demos and also raising the respectability of them. <br>This said, I totally agree with you! I wish that we could make the demos archival in themselves and not only talk about papers as archival and demos as "add-ons". But how can we make them archival? And how can we raise their status? This is a really, really important discussion to have. Right now, we are having it behind the scenes of CHI. I wish we could bring it out into the open. I believe that CHI will become less and less relevant unless we deal with this.</p><p>Kia</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Wednesday, March 14, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>Hi Kristina, thanks for the reply.</p><p>I don't think you can raise the status without making them reviewed at the same level as the papers, and without having a publication associated with them ... which means, to me, making them papers.</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Wednesday, March 14, 2012, UTC, Ed Chi said:</span>
  <span class='text'><p>As the other TPC chair, I also absolutely agree with many of your points.  I think SIGGRAPH has actually struck a better balance them SIGCHI has in this regard:</p><p>- System papers should be able to stand on its technical design merits;</p><p>- Demos should be given more academic credit.  We should get them to a point where we all list them on our CV in prominent places.</p><p>--Ed</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Wednesday, March 14, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>Ed, I would agree that SIGGRAPH accepts technical things more readily, but they have their own problems (i.e., it's super hard to get anything in that isn't rendering, animation or similar math-oriented things).  The "interactive systems" component of siggraph is laughably under-represented.</p><p>I would disagree that their demos get more academic credit.  A demo is a demo.  It's not really refereed so it gets listed in the "unrefereed backwater" of our CVs.  Nice to have, but not respected.  If all you do is demos, your dead.  You need papers.</p><p>Don't get me wrong on any of this:  I like demoing, and I do.  My lab has a game in the student game competition this year.  But, I'm not going to send a demo to CHI unless we also have a paper.  It's a waste of resources and time if it's not a "peer-reviewed contribution, by 3 or more reviewers" (to use the terminology we often see of CVs), that has a writeup with it that appears in the DL.</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Wednesday, March 14, 2012, UTC, Ed Chi said:</span>
  <span class='text'><p>My understanding is that some list SIGGRAPH demos as a prominent part of their resume (perhaps not by academics per se, but certainly I feel that others have taken SIGGRAPH interactive program more seriously than the CHI program, with them publishing a catalog and all in the past).</p><p>But your overall point is right.  A paper will always get counted more, and was something that the interactivity chairs this year tried to mitigate (by elevating the presence of demos during the program, and adding more rigor into the review processes.)</p><p>I think the larger fight is, as you suggested (and also suggested by others like James Landay), that we need to elevate technical system papers to the same status as papers with user studies. Even hardcore psych inspired HCI researchers like Stu Card agrees with this point of view.  He has a good talk on 'interaction science' that somewhat pushes on these points.</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Wednesday, March 14, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>I think you are confounding the Interactive part of SIGGRAPH and the SIGGRAPH Art Show.  The former, which has gone by many names (ETech, etc) and on which I've served as a jury member, is basically a glorified exhibit/demo;  it wouldn't go anywhere prominently on the CV, but it does get a lot of visibility at the show (which can have 10's of thousands of visitors).</p><p>The later is an art show (not a "demo") and is not aimed at academics;  rather it's the pervue of real artists (and typically has an art catalog in something like Leonardo, and has even resulted in books).  It is treated as a juried gallery by the art community, and is respected in some circles there.  There is no way CHI would/could/should aim for something like that.</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Thursday, March 15, 2012, UTC, Ed Chi said:</span>
  <span class='text'><p>A quick cursory search for "SIGGRAPH emerging technologies CV" turns up Pranav Mistry and Patrick Baundisch amongst others who list eTech as part of their CV.</p><p>Besides, let's not forget the mother of demos by Engelbart, who demonstrated (pun intended) how a demo can really change the world.</p><p>We are not giving it the prominence it deserves, simply because we don't give it prominence.  An issue that the demo chairs, Kia, and myself, are trying to experiment with.</p><p>But the overall issue is: technology and system based papers as well as demos do not seem to get thru the review process and therefore the attention that they deserve, and we should work to change that culture.</p></span>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
  <span class='source'> on Thursday, March 15, 2012, UTC, Floyd Mueller said:</span>
  <span class='text'><p>I agree, and as co-chair for Interactivity at CHI this and next year, I might be able to add some insights hopefully.<br>What we have done this year at CHI Interactivity:</p><p>- make it easier for paper authors to also demo at Interactivity (for example, you did not have to create a new submission document). This seemed to have worked, as we got &gt;2x submissions compared to last year.</p><p>- we secured a budget that helped with more complex demos to be shown at CHI. This worked for some authors helping them to come to Austin, but obviously not for all.</p><p>- Your Interactivity submission (if not part of a paper, see above) DOES go into the Digital Library. However, it is not referred (although we did use external reviews) in the CHI sense, as nothing is referred at CHI except Full Papers and Notes. It is also not archival. Some argue strongly for making Interactivity referred (for your reasons expressed above), others argue against it (as you give copyright away and can't publish it anywhere else then).</p><p>So some of these points might help clarify things. I certainly believe we have the biggest Interactivity this year at CHI, and hopefully will be able to build on this next year!</p><p>Cheers,<br>Floyd</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Thursday, March 15, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>I think this is the best approach we can hope for.  I would actually fall on the side that says if it's "just" a demo (i.e., in Interactivity or SIGGRAPH ETech, with it's short writeup) it shouldn't count as refereed because it precludes a longer publication.   And, of course, having paper presenters do demos is obviously a good thing.</p><p>The point of my original rant is that, if the work is truly considered valued and seen as having a contribution, it should have a paper and presentation.  If there's no contribution, why pretend it's "valuable" to CHI?</p><p>CHI has long struggled with the diversity of the community, and has done all kinds of different tracks, etc.  The success has been mixed, often because the "traditionalists" don't what "their CHI" watered down with that "other stuff."  I don't see a good solution, but just hoping folks will come demo isn't going to really cut it.</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Thursday, March 15, 2012, UTC, danielle wilde said:</span>
  <span class='text'><p>I'm curious why the perception is that the demos are not peer reviewed?</p><p>I am one of the co-chairs of the Interactivity Explorations track at CHI this year. we went to quite  a lot of effort to assemble a cast of high quality reviewers appropriate to our track and went through a rigorous reviewing process (with a minimum of three reviews per submission) except where we invited works _and_ the authors were happy to be curated in (ie not go through the peer review process). in these exceptional cases the works will be marked "invited" when exhibited/demoed at CHI to distinguish them from the works that have been peer reviewed.</p><p>in Australia, where I am currently based, exhibitions are accepted as publications. there are different criteria to assess the weight of the publication (just as there are for book chapters/journal articles/full conference papers/extended abstracts etc). Personally I push demoing through, where possible (and appropriately peer-reviewed), under the exhibition criteria, and be mindful to speak to the value of live demos whenever the opportunity arises.</p><p>importantly I do make a distinction between a proof-of-concept research demo and a fully-fledged research outcome, where knowledge is embodied in the completed artefact. In my opinion such works warrant being championed as "publishable" and "published" when exhibited, and considered of equal (and in some cases even more) valuable than a written article, as they give access to knowledge that isn't otherwise readily available.</p><p>the publication of artworks process that I refer to is particular to fine art, and is being used more and more in design research. While adapting it to HCI is still sometimes considered a bit radical, I believe it is inevitable as value is increasingly understood and given to physical embodiments of work that afford embodied understanding in complement of what is provided in a written publication. again, I am speaking of instantiations where the knowledge is embodied in the artefact, and best accessed through embodied engagement.</p><p>I personally position myself across a number of disciplines including HCI, fine art and exploratory design. I am able to walk this tightrope because of a unique set of circumstances (I recently completed the first Fine Arts practice-based PhD at CSIRO, Australia's national Commonwealth Scientific and Industrial Research Organisation, for example, won the VC's award for my thesis (woohoo!), as well as the inaugural Prime Minister's Australia Award to undertake research in Japan during my PhD - I am the only artist researcher to be given this award to date). I hope to leverage these "things that can attract listening beyond my disciplines and fellow travellers" to set precedents for others who do not so easily straddle different frameworks of understandings of what research, practice-based, process-driven and performative research approaches might be.</p><p>Bringing such changes in culture and thinking into being requires a lot of effort, though I sincerely believe the effort is warranted, and longed for by many.</p><p>For those who do not yet fully understand the value of demos, or the importance of attributing/recognising their value throughout the entire system, (I include the slow-changing and antiquated administrative processes in this :)).... I believe it's only a matter of time and exposure.</p><p>yes, there are places where that value is already understood and supported, but "institutions" such as CHI lag in this regard. This lag is exactly why I believe researchers should fight for culture change within CHI, and is why the Interactivity Explorations track was created this year.</p><p>as I'm sure everyone knows, it's easy to do what works, to take the path of least resistance, but doing so increasingly diminishes relevance (research is after all supposed to be generation of _new_ knowledge). If we can remind people, or bring them present to the value of embodied experience and its role in knowledge generation (especially in "institutions" such as CHI), we can effect the necessary culture change. As the people in this conversation already know only too well, such knowledge is not lesser than that supported through qualitative and quantitive research methods, it is merely different. its lack in contexts such as CHI is an enormous lack in and for knowledge in general. Do we drop CHI because of this? no. we do what we can to change it from the inside. Hopefully the small steps we are managing to take this year will help to set the stage for more positive and dramatic actions for the future. The important thing is not simply to complain, but to try to work out how to take action. Even tiny actions can help to effectuate big changes.</p><p>danielle</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Thursday, March 15, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>Thanks for the thoughtful reply, Danielle.</p><p>While I agree with much of your sentiments (I have spent much of my career doing work that results in installations and demos, collaborating with folks in art, media and design), I can't help but think its a bit naive.  Perhaps I'm just jaded.  CHI (and ACM, and IEEE) are conferences whose primary attendee population is steeped in disciplines based on the scientific method.  Most of the academic attendees work is judged (at the end of the day) by a community far broader than CHI, and at multiple promotion points (and tenure) is judged by the whole university.</p><p>Exhibitions and demos are a plus, when there is evidence the quality is high.  Having a quality jury and reasonable standards (e.g., what's the acceptance rate?  What are the conditions and metrics?  Are these published?) are a good thing, and I applaud that.  BUT, it is "only" a "plus" ... if there aren't publications to go along with the demo and exhibit, at least in the "sciency" disciplines, it's never going to count for much.</p><p>I will also point out that, with the vast amount of work going on in the CHI discipline, I don't really think there is a chance of CHI becoming irrelevant.  Perhaps it will not be the pinnacle of HCI and Interactive Design;  a question that never gets asked in these contexts is "why do you care?" ... if CHI does what it does well, and there are other venues (TED, youtube, hipster interactive galleries, etc) that show off other kinds of work, why not let them do what they do well, and have CHI do what CHI does well?  Because, if the "CHI paper" becomes unrecognizable (e.g., already, we see people submitting CV's where they liberally mix different CHI presentations together and the only way you can tell a note from a paper from a demo is by looking at the page count or going to the ACM DL), then the conference will become irrelevant as the core constituency abandons it.</p><p>So, keep up the good work, but keep reasonable goals and perspective. ;)</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Friday, March 16, 2012, UTC, danielle wilde said:</span>
  <span class='text'><p>thanks Blair.</p><p>while I certainly understand your position, in my experience setting reasonable goals leads to reasonable results, whereas being unreasonable can help people to break through ceilings, and bypass limiting prejudices - about themselves and others - and achieve extraordinary results.</p><p>research requires risk-taking, not knowing the answers, and sometimes being stupidly unreasonable. I know many of our institutions and funding bodies often forget this, but I sincerely believe that it's important that we researchers do not.</p><p>In response to the acceptance rate of the demos at CHI this year, it should be around 50% (not everything is finalised yet). I don't know if you will be there, but I hope if you are you will find the quality to be extremely high.</p><p>and finally, why do I care if CHI acknowledges the value of, and supports the presence of fully-fledged demos? well, that's easy. There are a whole lot of people at CHI developing the technologies that will help to shape our world. If they are supported to understand, value, access and be influenced by embodied engagement with research outcomes, then I believe the world they are helping shape might resemble a little more the world I would like to be living in.</p><p>best<br>danielle</p></span>
</li>
<li>
  <span class='source'> on Friday, March 16, 2012, UTC, Ed Chi said:</span>
  <span class='text'><p>I agree with several of your points, Blair, but one I cannot:<br>"if CHI does what it does well, and there are other venues (TED, youtube, hipster interactive galleries, etc) that show off other kinds of work, why not let them do what they do well, and have CHI do what CHI does well?"</p><p>I'm against this sentiment for a number of reasons:</p><p>- If we're serious about being the premier place where people learn about the frontier of interaction and HCI, then we cannot afford to ignore anything relating to it.  For it not to appear at CHI would mean we're not serious about our claim of being the best place to showcase the best work.</p><p>Increasingly, we see the most innovative work on systems being shown in other places other than CHI, and I consider this a real problem that would lead to the decline of CHI as a premier place for the interaction frontier (for science or engineering).</p><p>- Perhaps even more seriously, I think it is very easy to become closed and stale and not bring in new ideas.  Overtime, this type of sentiment is damaging to a field, IMHO.  Here I think of Database's community stance toward unstructured datastores and becoming less relevant in the world of big data; or the Hypertext community's missing of the world wide web research in the first decade or so.</p><p>I believe the facts are that:<br>History, over and over again, has shown that communities that are open survive over the long term and prosper by being inclusive of new ideas.  Do the opposite, and you die a slow death.</p><p>PS: interaction magazine folks are welcome to publish my comments here with attribution (possibly requiring some editing.) :)</p></span>
</li>
</ul>
</li>
</ul>
</li>
<li>
  <span class='source'> on Tuesday, April 10, 2012, UTC, Luis Leiva said:</span>
  <span class='text'><p>I couldn't agree more with Ed's viewpoint(s). In high quality HCI conferences, demonstrations should be particularly featured, peer-reviewed, and archival material that gets published in the main conference proceedings. UIST and IUI are already doing this. Moreover, at WWW, demos are presented in a short/full paper one-to-many audience setting. CHI should go in this direction to attract more tech-oriented researchers. Indeed, I feel this is the 'missing part' at CHI.</p></span>
</li>
<li>
  <span class='source'> on Wednesday, April 18, 2012, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>Ed, I guess it's a question of viewpoint.  CHI cannot be all things to all people.  It's already a bit to big and a bit of a circus, and it's silly to imply that all high quality HCI work could be hosted at CHI.  The conference would have to grow to such a size and have so many parallel tracks that no given person would see much of anything.  (although, riffing on that idea:  instead of making "CHI" everything, why not follow Viz and convince UIST/3DUI/etc to collocated and create a "CHI Week" of collocated events?)</p><p>My main point is still that, if CHI doesn't consider work "good enough" to be accepted, promoted and presented as a full fledged top-tier contribution (that I, as a researcher, can put on my CV as such, and will be perceived by my peers as such), then expecting folks to come and demo is misguided and silly.</p><p>If CHI had a full-fledged tech track, with high-quality content across the spectrum of CHI research, I'd submit and go.  If my work was accepted, I'd present.  But, I have other ways to have impact with my demos (in addition to publication at the small conferences) that are better than being reception entertainment at CHI:<br>- show videos on youtube (one of mine is almost at 1 million views, and has clearly had more impact on the world through that than any CHI demo ever will)<br>- distribute games and apps for free through the mobile device stores<br>- give talks at more industry oriented conferences (GDC, CES, Qualcomm's Uqlinq, VC gatherings, etc)</p><p>That said, if CHI does create compelling events, we'll come.  We have one of the 3 finalist games in the CHI game competition this year, and the students are really looking forward to presenting and demoing.  (I can't go for what are obvious reasons to folks who know our family).</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2009/03/02/microsoft-ar-envisionment/</div>
<span id='oldComments'>
<h2>1 Archived Comment</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Tuesday, February 9, 2016, UTC, baselo sisil said:</span>
  <span class='text'><p>This article is very good and useful for readers. thank you for sharing knowledge . <a href="http://obatpenggugurkandungan.info" rel="nofollow noopener" title="http://obatpenggugurkandungan.info">cara menggugurkan kandungan</a></p></span>
</li>
</ul>
</span>






<div class='commentURL'>/2017/05/02/quick-blog-posts-from-ios/</div>
<span id='oldComments'>
<h2>2 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Thursday, September 7, 2017, UTC, Chaitanya. Katepalli said:</span>
  <span class='text'><p>how to retracted one phone for one user and how to know there first time install the app and multi time install the app..</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Thursday, September 7, 2017, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>I am not sure I see what you are asking.</p></span>
</li>
</ul>
</li>
</ul>
</span>

<div class='commentURL'>/2017/05/20/its-not-webar-yet/</div>
<span id='oldComments'>
<h2>6 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, May 22, 2017, UTC, Iker Jamardo Zugaza said:</span>
  <span class='text'><p>Great dissertation on how WebAR could evolve. I agree that the User Agent needs to handle more and only expose functionality as needed. I also agree that extending some of the functionalities of the WebVR API is the right way to go (like extending the VRLayer structures) and that the 3 pilars (platform independency, performance and privacy) are essential goals we all should pursue. The only issue this article should cover (in my opinion) is the important fact that the Chromium WebAR prototype lays a foundation to build many of the elements that are commented here. The WebVR specification is going through significant changes for its release version 2.0 that will affect many of these discussions. This is not an excuse to say that we cannot have conversations as where the WebAR API should evolve to. But my take is this: By exposing low level functionalities, we can have a conversation about possible API routes in a much meaningful way as many of the elements discussed here could be implemented using polyfills. Want a VRLayer that renders the camera? Want to check it feasability? With one of the main characteristics of JavaScript this is a possibility. Maybe not everything can be achieved, and when this time comes, the modifications to the UA can be proposed. But modifying the UA is not trivial and takes time, so my opinion is that proposals should be well thought. The current proposal is not perfect and is not meant to be, but I deeply think/hope it will unleash both a healthy conversation about the spec and also allow to start building experiences on top (as important as the spec is to start building use cases that might highlight needs in the spec itself).</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, May 22, 2017, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>I agree that we can experiment with polyfills.  And, that others could experiment with these three pillars by taking the code and trying out different ideas for doing that.  That would be great, I hope people do!   We hope to add support for this into argon.js and do some of that work.</p><p>This Chromium demo, as "an AR extension for WebVR to spur discussions", is great: it creates the necessary infrastructure for experimenting with what one might want or need to do.</p><p>But, I was responding specifically to the API as a model for how WebAR should work, and things folks should think about when building on this work.  The Chromium demo as "Proposal for WebAR" (which is what the README says, and what what people are therefore talking about) does not do that.  The API "proposes", based on the methods and structure that is exposed, that WebAR should be based around the idea of sending video and sensor data into the web page for rendering, and I think this is a fundamentally wrong way to implement WebAR.   And since the web page asked for a discussion of this as a proposal for WebAR, I discussed it that way. :)</p><p>You and I understand the tradeoffs on implementation approaches:  we've chatted about them in the past. There are limitations by not giving the web page access to video (e.g., some graphical effects are impossible), so I can understand the desire to provide it. Not providing the video also limits ways things could be rendered in the page.  But, this prototype could equally have rendered the video (and perhaps depth info) into the canvas inside the Chromium extension (before calling rAF).</p><p>I think the demo is great, as I said in the article.  But I want people who use it and experiment with it to be thinking about these issues.</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Friday, August 4, 2017, UTC, Djela said:</span>
  <span class='text'><p>Enjoyed your blog as it raises the question of context. Why an XR experience should adapt to diverse user's context? It is something to consider in the design of such experiences. <br>Also, along representations of the world proposed through StreetView or OpenStreetMap, I would also consider Mapillary and the Mapillary API <a href="https://www.mapillary.com/developer" rel="nofollow noopener" title="https://www.mapillary.com/developer">https://www.mapillary.com/d...</a>. They have been very creative developing a Structure from Motion workflow (3D reconstruction from user generated 2D images of streets and cities) with OpenSfM <a href="https://github.com/mapillary/OpenSfM" rel="nofollow noopener" title="https://github.com/mapillary/OpenSfM">https://github.com/mapillar...</a>. That could count as another source of “realities” for argon.js - for which I also thank you very much! <br>Cheers!</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Saturday, August 5, 2017, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>Thanks for the pointer to Mapillary, I hadn't seen that, we'll check out.  Looks like something we should support, and can make good use of!</p><p>Can you give me an example of what you are thinking about with respect to diverse user's context?  I think adapting to different users to give them the best possible experience is probably a dream of most developers, but getting access to content to help with that adaptation is the challenge.  What do you imagine? </p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, August 7, 2017, UTC, Djela said:</span>
  <span class='text'><p>Thanks for your question.</p><p>Well, for a start, we could imagine a sort of responsive adaptation to user’s context by getting more granular with the elements of a mixed reality experience. Again, going back to the Mapillary breach into computer vision, the second aspect of their approach (aside from structure from motion and point cloud creation) is semantic segmentation, « automatic tagging for each pixel ». The questions for me would be: what are the the elements (gestures, motions, sonic, visual features, etc.) at the core of a given experience? How could a web system (browser or else) learn from and adapt to those core elements? Which resources could this system deploy to render an appropriate view of reality following user’s choice and respecting the core elements of the XR app?</p><p>For example, the representation of reality could be extracted and generated from point cloud data from user generated geotagged photos. The web system would then be charged with matching the core elements of the mixed reality experience with this given reality. As in a continuum from VR to AR, the user could then control how dense and how close to real-time this reality is. Because, as you said in your blog post, this representation could also be a construct made of archives (e.g. StreetView). Also, building on the automatic semantic segmentation, some machine learning could help make the matching more adaptative and communicate to the user what is perceived from its context. Although, in the end, isn’t the user’s context up to the user to control and communicate?</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, August 7, 2017, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>I get what you are suggesting, that kind of adaptation is the sort of thing that has motivated folks doing adaptive computing (in Ubicomp, AR and so on) for years.  The problem is, this kind of understanding of visual scene and the user's context is notoriously hard to get right, and when the system gets it wrong, it break the illusion ... badly.  I think as this kind of scene understanding gets better, and we figure out what kinds of context actually matters, it'll be possible to do this sort of thing.  I don't really see it happening any time soon, though.</p></span>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</span>

<div class='commentURL'>/2017/06/14/valid-ssl-certs-for-local-https-webdev/</div>
<span id='oldComments'>
<h2>1 Archived Comment</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Wednesday, August 16, 2017, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>And for those that care, I just did an early attempt at renewing things, and it was "not terrible" (although also "not awesome").  Running "sudo certbot certonly --manual --preferred-challenges dns --cert-name <a href="http://profblair.bmaci.com" rel="nofollow noopener" title="profblair.bmaci.com">profblair.bmaci.com</a>" gave me a new set of challenge values to put on my DNS server;  updating the eight TXT entries was tedious but not terrible ... I definitely need to figure out a more automated approach for the future, but I guess this will do for now.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2017/11/14/what-is-an-mr-blog/</div>
<span id='oldComments'>
<h2>2 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, November 20, 2017, UTC, Rodney Berry at CHOICE said:</span>
  <span class='text'><p>I was thinking about Bluebrain's National Mall project in 2011 where they released their album as an app so that each track could only be heard at particular locations around the National Mall in Washington DC. <a href="http://bluebrainmusic.blogspot.com.au/2011/03/national-mall.html" rel="nofollow noopener" title="http://bluebrainmusic.blogspot.com.au/2011/03/national-mall.html">http://bluebrainmusic.blogs...</a><br>It strikes me that things could be linked not only to specific locations but to certain classes of locations. For example, what might be in a blog that could be accessed from any toilet cubicle in the world but only from a toilet cubicle?</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Monday, November 20, 2017, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>I agree, broadly being able to associate content with all kinds of context (time, location, objects, people, activity, etc) would be great.  So, I could cause different parts of the blog to appear in different places, without being "3D" and attached.  But I still feel like there's some big holes in getting from here (2D) to there ...</p></span>
</li>
</ul>
</li>
</ul>
</span>

<div class='commentURL'>/2018/01/18/food-blogs-and-the-future-of-mr-on-the-web/</div>
<span id='oldComments'>
<h2>6 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Friday, January 19, 2018, UTC, Arturo Paracuellos said:</span>
  <span class='text'><p>I agree that photogrammetry/easy ways of creating 3D content will be one of the keys of the future of web based MR.<br>For example Sony has a promising device XZ1 which allows you capture real objects and to port to web in a very simple way <a href="https://youtu.be/lOVKgOAlCdg?t=1m28s" rel="nofollow noopener" title="https://youtu.be/lOVKgOAlCdg?t=1m28s">https://youtu.be/lOVKgOAlCd...</a><br>I think with this kind of tools, and if we develop easy to use 3D editors to be able to create/remix 3D content with touchscreen devices for example, along with well designed interactive embed containers that are easy to integrate into html pages, and useful from browser desktop, through touchscreen devices, to VR and AR supported devices, we could make a great ecosystem to have awesome MR experiences accesible for a huge audience.</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Friday, January 19, 2018, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>Nice;  it'll be great when there are more examples of this sort of thing!</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Wednesday, August 22, 2018, UTC, Camila6323 said:</span>
  <span class='text'><p><a href="https://uploads.disquscdn.com/images/b3f4cc250cd47b45e8a1e23ce000cebe2e677e9abd5ed37f7d8b7feb42300f5f.jpg" rel="nofollow noopener" title="https://uploads.disquscdn.com/images/b3f4cc250cd47b45e8a1e23ce000cebe2e677e9abd5ed37f7d8b7feb42300f5f.jpg">https://uploads.disquscdn.c...</a> I asked my brother why he was taking a dictionary and thesaurus to his theater rehearsal. He said it was a play on words.</p></span>
</li>
<li>
  <span class='source'> on Saturday, August 25, 2018, UTC, Persephone6239 said:</span>
  <span class='text'><p><a href="https://uploads.disquscdn.com/images/14ce72b5b08e47676c2dfe6ebcbbb8749dd7d7579df0713c7fbe7db4337edd41.jpg" rel="nofollow noopener" title="https://uploads.disquscdn.com/images/14ce72b5b08e47676c2dfe6ebcbbb8749dd7d7579df0713c7fbe7db4337edd41.jpg">https://uploads.disquscdn.c...</a> Now..a cheaper way to express your love...-E-cards !</p></span>
</li>
<li>
  <span class='source'> on Monday, August 27, 2018, UTC, Crystal9529 said:</span>
  <span class='text'><p><a href="https://uploads.disquscdn.com/images/790ccd673af7b29321a42701615315c8bd35e50c9f64a3a1ba46b6a606dcc76f.jpg" rel="nofollow noopener" title="https://uploads.disquscdn.com/images/790ccd673af7b29321a42701615315c8bd35e50c9f64a3a1ba46b6a606dcc76f.jpg">https://uploads.disquscdn.c...</a> What's the difference between a peeping Tom and a pickpocket? A peeping Tom snatches watch's</p></span>
</li>
<li>
  <span class='source'> on Tuesday, August 28, 2018, UTC, Nina4971 said:</span>
  <span class='text'><p><a href="https://uploads.disquscdn.com/images/6d2928bb633c486f8bec06805f825edd18814fffc61d28d02ea8ab72a40667b8.jpg" rel="nofollow noopener" title="https://uploads.disquscdn.com/images/6d2928bb633c486f8bec06805f825edd18814fffc61d28d02ea8ab72a40667b8.jpg">https://uploads.disquscdn.c...</a> Me: What does that cloud look like to you? 3-year-old: A cloud. Me: No, what do you imagine it could be? 3-year-old: Rain.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2018/05/04/webxr-cv</div>
<span id='oldComments'>
<h2>4 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Wednesday, January 9, 2019, UTC, roberto solini said:</span>
  <span class='text'><p>Thanks for this article.<br>I wonder if there's any feature about computer vision in webxr.<br>Would we have face or object detection capacity in webxr ?</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Wednesday, January 9, 2019, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>There isn't anything yet.  The initial release of WebXR (over on <a href="http://github.com/immersive-web/webxr" rel="nofollow noopener" title="github.com/immersive-web/webxr">github.com/immersive-web/webxr</a> ) will not support any sort of camera access.  It will come eventually, though.</p></span>
</li>
</ul>
</li>
<li>
  <span class='source'> on Friday, February 15, 2019, UTC, sylvain cordier said:</span>
  <span class='text'><p>Hi Blair.<br>Do you think that webxr will support marker tracker in the future or will we be force to use external computer vision library like opencv ?<br>Is there any chance that anchors evolve to allow that ?</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Friday, February 15, 2019, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>I think WebXR will eventually provide access to a lot of native sensing features.  The big challenge is figuring out what to expose, and how, since the platforms are very diverse right now.  For example, I would love to support some very simple things (QRCodes of various forms, simple images with a certain kind of feature set), but first folks are anxious to get the basic API done.</p></span>
</li>
</ul>
</li>
</ul>
</span>

<div class='commentURL'>/2018/05/12/gdpr-privacy-notices</div>
<span id='oldComments'>
<h2>1 Archived Comment</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Thursday, May 24, 2018, UTC, David Spector said:</span>
  <span class='text'><p>There should be a universal choice one can make to receive an abbreviated email saying "Company XYZ is now complying with the GDPR regulations." Even better, there could be a single alphabetized text file on the Web listing all organizations that claim to be compliant, with NO more emails sent. Since that file would be unreasonably long, it could be wrapped in a database--one would query it to find out if XYZ claims to be compliant. The burden should be borne by those who care to know, not by all customers of all companies in the world.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2018/05/14/-canary-webxr</div>
<span id='oldComments'>
<h2>2 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Friday, June 8, 2018, UTC, Pretzel and Vodka said:</span>
  <span class='text'><p>I've been playing around with WebXR/ARcore/ARtoolkit/aframe-ar, AR.js and Argon for a while now, trying to get the right solution for my long-term project. Although I'm getting more and more convinced I can realise it with Web rather than a native app, I'm still deperately missing a solution to recognize logos/images or at least QR codes as markers, like the Vuforia recognition integrated into Argon (just without the custom browser). It's nice to have a Porsche driving around my floor and place astronauts on my desk, but for me a good usage for AR in the web might rather be attaching information to real world products.</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Saturday, June 9, 2018, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>This will happen, I'm pretty sure of it.  I can't comment on when it will happen in all browsers, but most of the pieces are (or can be) in place.  WebXR will start appearing in other browsers beyond Chrome Canary soon, in both the Canary-like-versions and eventually real versions.  But probably not till the fall or winter.</p></span>
</li>
</ul>
</li>
</ul>
</span>

<div class='commentURL'>/2018/06/25/ar-property-rights</div>
<span id='oldComments'>
<h2>1 Archived Comment</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Friday, November 16, 2018, UTC, John H. Yang said:</span>
  <span class='text'><p>I like this framework/approach on this emerging issue. To take it one step further, when I think about these new property rights, I think about a new form of monetization that becomes available for these property rights owners. Imagine ADT paying homes to enable AR-based ADT branding on their house. And now, people with AR-enabled devices will see ADT badging on those houses.</p></span>
</li>
</ul>
</span>

<div class='commentURL'>/2018/06/29/camera-access-webxr</div>
<span id='oldComments'>
<h2>2 Archived Comments</h2>
<ul class='comments archived'>
<li>
  <span class='source'> on Saturday, January 12, 2019, UTC, sonia maklouf said:</span>
  <span class='text'><p>Hi Blair</p><p>I read your post about camera access in webxr and I want you ask you a question.</p><p>I use an online product configurator which give me an iframe code that I embed in my website with webxr it will enable  augmented reality which is great but the iframe point out to their company via src and I wonder if it will cause security issue like could they control user's camera ?</p><p>Thank you for posting about this subject because webxr is supposed to be include in web browser this year (I guess so I've seen Expected completion: Q4 2019 in <a href="https://www.w3.org/2018/09/immersive-web-wg-charter)" rel="nofollow noopener" title="https://www.w3.org/2018/09/immersive-web-wg-charter)">https://www.w3.org/2018/09/...</a></p><p>              and I found very hard to find post about camera and how it show be managed</p></span>
<ul class='comments archived'>
<li>
  <span class='source'> on Sunday, January 13, 2019, UTC, Blair MacIntyre said:</span>
  <span class='text'><p>I'm not sure if this has been decided on, but I suspect embedding WebXR in iFrames (especially ones that point to difference domains) will cause issues on most browsers because of how that capability could be abused.</p><p>WebXR currently has no way to access or manage the camera.  I suspect that this might end up happening by coordinating enhancements with WebRTC.</p></span>
</li>
</ul>
</li>
</ul>
</span>

